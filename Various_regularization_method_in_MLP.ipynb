{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Various regularization method in MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lFTKCwlNNqA",
        "colab_type": "text"
      },
      "source": [
        "1. ##    1. Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BrnuIOR7Phz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2360e528-4f9c-47a0-d9bd-8dff5b1ca16b"
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "# import torchvision.datasets as dset\n",
        "# import torchvision.transforms as transforms\n",
        "# import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from itertools import product\n",
        "# Set random seed for reproducibility\n",
        "manual_seed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "!mkdir results"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  2529\n",
            "mkdir: cannot create directory ‘results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6edMlFWNZnM",
        "colab_type": "text"
      },
      "source": [
        "## 2. Downloading Dataset\n",
        "\n",
        "Here, I will use Fashion MNIST as out input data set. It is image data set where each image is clothes. \n",
        "![Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
        "\n",
        "Each data is 28×28 with grayscale image associated with 10 class labels, total 784 dimensions for input size.\n",
        "\n",
        "0.\tT-shirt/top\n",
        "1.\tTrouser\n",
        "2.\tPullover\n",
        "3.\tDress\n",
        "4.\tCoat\n",
        "5.\tSandal\n",
        "6.\tShirt\n",
        "7.\tSneaker\n",
        "8.\tBag\n",
        "9.\tAnkle boot\n",
        "\n",
        "\n",
        "The data set contains total 60,000 data for training and 10,000 data for testing. Here I will use 50,000 among training set for training and 10,000 for validation, and finally 10,000 for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dVz8X8-PSQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "cc92e970-085d-467f-a2ed-411aa9ee6de8"
      },
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(), \n",
        "     torchvision.transforms.Normalize((0,), (0.5,))])\n",
        "# Need to add normalize\n",
        "trainset = torchvision.datasets.FashionMNIST(root='../data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST(root='../data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "partition = {'train+val': trainset, 'test':testset}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:04, 5865455.51it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 39870.20it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:02, 1683046.56it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 15078.95it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LOfN27IPmwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "22a5dba1-d86a-418a-ead7-e3ddbcf2aa7a"
      },
      "source": [
        "print(len(partition[\"train+val\"]))\n",
        "print(len(partition[\"test\"]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mxhsG8SPlKT",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model of neural network\n",
        "\n",
        "Here, I will use MLP with batch normalization and dropout. I will fix size of neural net as 576-500-500-500-500-10 so we get enough overfitting, and regularization method works well. \n",
        "\n",
        "Also, I will apply weight initialization method. Here I will use xavier init for tanh/sigmoid activation function, and He init for ReLU activation function. For this choice, I referred this article [Weight Initialization](https://reniew.github.io/13/). \n",
        "\n",
        "One of the regularization method we studied was ridge/lasso regularization. However, it is not implemented in error function, because weight_decay in optimizer's field works exactly same as l2 regularization. So I will use that for another regularization method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCwWUdJoQOaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, batchnorm):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout\n",
        "        self.batchnorm = batchnorm\n",
        "        \n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        self.init_bn = nn.BatchNorm1d(self.hid_dim)\n",
        "        self.last_bn = nn.BatchNorm1d(self.out_dim)\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.batchnorm:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "                \n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "        \n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act == nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "        \n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.weight_init()\n",
        "          \n",
        "    def forward(self, x):\n",
        "        # Ordering : FC -> BatchNorm -> act -> dropout\n",
        "        # I referred this stackoverflow answer for ordering. \n",
        "        # https://stackoverflow.com/a/40295999\n",
        "        x = self.fc1(x)\n",
        "        x = self.init_bn(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(len(self.linears)):\n",
        "            x = self.linears[i](x)\n",
        "            if self.batchnorm: \n",
        "                x = self.bns[i](x)\n",
        "            x = self.act(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.last_bn(x)\n",
        "        x = self.act(x)\n",
        "        # No dropout in last\n",
        "        return x\n",
        "    \n",
        "    def weight_init(self):\n",
        "        for linear in self.linears:\n",
        "            if self.act == 'tanh' or self.act == 'sigmoid':\n",
        "                nn.init.xavier_normal_(linear.weight)\n",
        "            else:\n",
        "                nn.init.kaiming_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)\n",
        "            \n",
        "test_net = MLP(in_dim = 784, out_dim = 10, hid_dim = 100, n_layer = 4, act = 'sigmoid', dropout = 0.1, batchnorm = True) # Testing Model Construction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfdZCS_ZRKE9",
        "colab_type": "text"
      },
      "source": [
        "## 4. Defining Hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_dMgMN2RL26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "39b881be-4511-4c96-a88e-10a4cc5c4caf"
      },
      "source": [
        "# parser declaration\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.in_dim = 784 # Fixed\n",
        "args.out_dim = 10 # Fixed\n",
        "args.batch_size = 8 # Fixed\n",
        "args.test_batch_size = 1000 # Fixed\n",
        "args.lr = .01 # Scheduler will change\n",
        "args.epoch = 21 # Fixed\n",
        "args.hid_dim = 300 # Fixed\n",
        "args.n_layer = 4 # Fixed\n",
        "args.dropout = .1 # tuned\n",
        "args.batch_norm = True # tuned\n",
        "args.act = 'relu' # tuned\n",
        "args.step_size = 3 # Fixed\n",
        "args.gamma = 1/np.sqrt(10) # Fixed\n",
        "args.weight_decay = 1 # tuned\n",
        "\n",
        "print(args)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(act='relu', batch_norm=True, batch_size=8, dropout=0.1, epoch=21, gamma=0.31622776601683794, hid_dim=300, in_dim=784, lr=0.01, n_layer=4, out_dim=10, step_size=3, test_batch_size=1000, weight_decay=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDQf7ehjfrCJ",
        "colab_type": "text"
      },
      "source": [
        "## 5. Defining training related function, train and validate, test.\n",
        "\n",
        "Here, I used cross validation. Unlike cross validation on our lecture slide, I randomly sampled 50,000 data for training for each epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phUt6MFbY5mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, args.in_dim)\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        assert(not np.isnan(loss.item()))\n",
        "        #print(loss.item())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ0N9Dgnfxfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, args.in_dim)\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            assert(not np.isnan(loss.item()))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqAM_AEmf0Bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, args.in_dim)\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykobWlRhf4QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.batchnorm)\n",
        "    # net.cuda()\n",
        "    print(args)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, args.step_size, args.gamma)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        trainset, valset = torch.utils.data.random_split(partition[\"train+val\"], [50000, 10000])\n",
        "        net, train_loss, train_acc = train(net, {\"train\":trainset, \"val\":valset, \"test\":partition[\"test\"]}, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, {\"train\":trainset, \"val\":valset, \"test\":partition[\"test\"]}, criterion, args)\n",
        "\n",
        "        te = time.time()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    \n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGADMrWgcsn",
        "colab_type": "text"
      },
      "source": [
        "## 6. Plotting & Saving experiment result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COt-ABXQgbgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoSVSLc3jB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
        "    \n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    ax[2].set_title('Test Accuracy')\n",
        "\n",
        "    \n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQvKJVVYgmIs",
        "colab_type": "text"
      },
      "source": [
        "## 7. Experiment\n",
        "\n",
        "My experiment will proceed as following. Our hyperparameter tuning will focus on 3 parameters, dropout, batchnorm, weight_decay(works for l2 regularization). \n",
        "\n",
        "The goal of this experiment is minimizing validation loss. \n",
        "\n",
        "The hyperparameter tuning will be grid tested for following list.\n",
        "\n",
        "dropout : \\[0.0, 0.1, 0.2, 0.3, 0.4\\]\n",
        "\n",
        "batchnorm : \\[True, False\\]\n",
        "\n",
        "weight_decay : \\[$10^{-2}, 10^{-1}, 1, 10^1, 10^2$\\]\n",
        "\n",
        "For learning rate, I will use learning rate scheduler in following schedule.\n",
        "\n",
        "| epoch   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     | lr          |\n",
        "|--------------|-------------|\n",
        "| $0\\le e<3$   | $10^{-2}$   |\n",
        "| $3\\le e<6$   | $10^{-2.5}$ |\n",
        "| $6\\le e<9$   | $10^{-3}$   |\n",
        "| $9\\le e<12$  | $10^{-3.5}$ |\n",
        "| $12\\le e<15$ | $10^{-4}$   |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL1lPQurgkuP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "bfb77660-b481-4b91-f840-95d4d9389786"
      },
      "source": [
        "args.exp_name = \"exp1\"\n",
        "\n",
        "name_var1 = 'weight_decay'\n",
        "name_var2 = 'dropout'\n",
        "name_var3 = 'batchnorm'\n",
        "list_var1 = [.1, 1, 10]\n",
        "list_var2 = [.0, .1, .2, .3, .4]\n",
        "list_var3 = [True, False]\n",
        "\n",
        "for var_list in product(list_var1, list_var2, list_var3):\n",
        "    setattr(args, name_var1, var_list[0])\n",
        "    setattr(args, name_var2, var_list[1])\n",
        "    setattr(args, name_var3, var_list[2])\n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(act='relu', batch_norm=True, batch_size=8, batchnorm=True, dropout=0.0, epoch=21, exp_name='exp1', gamma=0.31622776601683794, hid_dim=300, in_dim=784, lr=0.01, n_layer=4, out_dim=10, step_size=3, test_batch_size=1000, weight_decay=0.1)\n",
            "Epoch 0, Acc(train/val): 16.75/11.70, Loss(train/val) 2.29/2.48. Took 99.05 sec\n",
            "Epoch 1, Acc(train/val): 14.54/9.84, Loss(train/val) 2.37/2.69. Took 99.79 sec\n",
            "Epoch 2, Acc(train/val): 13.08/10.02, Loss(train/val) 2.47/4.30. Took 99.55 sec\n",
            "Epoch 3, Acc(train/val): 12.38/9.89, Loss(train/val) 2.39/2.48. Took 100.13 sec\n",
            "Epoch 4, Acc(train/val): 18.06/13.29, Loss(train/val) 2.25/2.32. Took 100.14 sec\n",
            "Epoch 5, Acc(train/val): 28.96/11.55, Loss(train/val) 2.07/2.30. Took 99.71 sec\n",
            "Epoch 6, Acc(train/val): 38.90/45.85, Loss(train/val) 1.90/1.71. Took 99.47 sec\n",
            "Epoch 7, Acc(train/val): 37.82/47.83, Loss(train/val) 1.92/1.69. Took 99.73 sec\n",
            "Epoch 8, Acc(train/val): 37.18/45.64, Loss(train/val) 1.93/1.85. Took 99.73 sec\n",
            "Epoch 9, Acc(train/val): 41.73/50.94, Loss(train/val) 1.85/1.93. Took 99.47 sec\n",
            "Epoch 10, Acc(train/val): 45.77/61.62, Loss(train/val) 1.77/1.75. Took 99.17 sec\n",
            "Epoch 11, Acc(train/val): 47.43/57.02, Loss(train/val) 1.74/1.75. Took 99.43 sec\n",
            "Epoch 12, Acc(train/val): 52.34/69.17, Loss(train/val) 1.65/1.62. Took 99.45 sec\n",
            "Epoch 13, Acc(train/val): 54.74/67.68, Loss(train/val) 1.60/1.51. Took 99.47 sec\n",
            "Epoch 14, Acc(train/val): 55.57/73.80, Loss(train/val) 1.58/1.52. Took 99.90 sec\n",
            "Epoch 15, Acc(train/val): 59.39/62.23, Loss(train/val) 1.52/1.31. Took 99.60 sec\n",
            "Epoch 16, Acc(train/val): 58.39/74.10, Loss(train/val) 1.52/1.28. Took 99.60 sec\n",
            "Epoch 17, Acc(train/val): 58.46/71.22, Loss(train/val) 1.53/1.24. Took 99.31 sec\n",
            "Epoch 18, Acc(train/val): 60.24/77.45, Loss(train/val) 1.49/1.25. Took 99.28 sec\n",
            "Epoch 19, Acc(train/val): 60.53/75.49, Loss(train/val) 1.49/1.21. Took 100.13 sec\n",
            "Epoch 20, Acc(train/val): 60.09/76.16, Loss(train/val) 1.49/1.27. Took 101.03 sec\n",
            "Namespace(act='relu', batch_norm=True, batch_size=8, batchnorm=False, dropout=0.0, epoch=21, exp_name='exp1', gamma=0.31622776601683794, hid_dim=300, in_dim=784, lr=0.01, n_layer=4, out_dim=10, step_size=3, test_batch_size=1000, weight_decay=0.1)\n",
            "Epoch 0, Acc(train/val): 15.72/10.27, Loss(train/val) 2.32/2.66. Took 64.22 sec\n",
            "Epoch 1, Acc(train/val): 14.42/12.55, Loss(train/val) 2.38/2.63. Took 65.13 sec\n",
            "Epoch 2, Acc(train/val): 13.32/5.36, Loss(train/val) 2.44/3.03. Took 63.72 sec\n",
            "Epoch 3, Acc(train/val): 14.43/19.19, Loss(train/val) 2.36/2.47. Took 64.31 sec\n",
            "Epoch 4, Acc(train/val): 30.47/9.95, Loss(train/val) 2.05/2.87. Took 62.69 sec\n",
            "Epoch 5, Acc(train/val): 31.44/14.93, Loss(train/val) 2.03/2.38. Took 63.31 sec\n",
            "Epoch 6, Acc(train/val): 42.02/31.60, Loss(train/val) 1.85/2.03. Took 64.26 sec\n",
            "Epoch 7, Acc(train/val): 41.04/49.42, Loss(train/val) 1.86/1.89. Took 65.13 sec\n",
            "Epoch 8, Acc(train/val): 38.73/28.30, Loss(train/val) 1.90/1.98. Took 65.70 sec\n",
            "Epoch 9, Acc(train/val): 41.80/61.27, Loss(train/val) 1.85/1.54. Took 65.22 sec\n",
            "Epoch 10, Acc(train/val): 46.28/53.66, Loss(train/val) 1.78/1.73. Took 66.65 sec\n",
            "Epoch 11, Acc(train/val): 50.87/63.97, Loss(train/val) 1.72/1.60. Took 64.32 sec\n",
            "Epoch 12, Acc(train/val): 55.33/49.82, Loss(train/val) 1.66/1.75. Took 64.75 sec\n",
            "Epoch 13, Acc(train/val): 57.43/56.24, Loss(train/val) 1.60/1.51. Took 64.29 sec\n",
            "Epoch 14, Acc(train/val): 60.73/47.05, Loss(train/val) 1.57/1.53. Took 62.34 sec\n",
            "Epoch 15, Acc(train/val): 62.77/71.50, Loss(train/val) 1.53/1.27. Took 65.97 sec\n",
            "Epoch 16, Acc(train/val): 63.20/68.37, Loss(train/val) 1.51/1.31. Took 65.05 sec\n",
            "Epoch 17, Acc(train/val): 63.08/70.51, Loss(train/val) 1.51/1.28. Took 63.92 sec\n",
            "Epoch 18, Acc(train/val): 63.72/68.60, Loss(train/val) 1.49/1.28. Took 64.41 sec\n",
            "Epoch 19, Acc(train/val): 63.89/71.66, Loss(train/val) 1.49/1.24. Took 64.06 sec\n",
            "Epoch 20, Acc(train/val): 63.34/69.78, Loss(train/val) 1.50/1.28. Took 64.70 sec\n",
            "Namespace(act='relu', batch_norm=True, batch_size=8, batchnorm=True, dropout=0.1, epoch=21, exp_name='exp1', gamma=0.31622776601683794, hid_dim=300, in_dim=784, lr=0.01, n_layer=4, out_dim=10, step_size=3, test_batch_size=1000, weight_decay=0.1)\n",
            "Epoch 0, Acc(train/val): 15.44/18.20, Loss(train/val) 2.33/2.26. Took 102.60 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}